{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Data Analysis: Machine Learning Assignment\n",
    "\n",
    "- Statistical Data Analysis (SPA6328)\n",
    "- Academic Year: 2020-2021\n",
    "- Module Organiser: Dr Seth Zenz\n",
    "- Module Associate: Prof Adrian Bevan\n",
    "\n",
    "Copyright (C) Queen Mary University of London\n",
    "\n",
    "## This assessment is for summative feedback.\n",
    "\n",
    "In this assignment you will analyse the iris data using decision tree based classifiers.  Specifically we are using the AdaBoost decision tree.  Each decision tree by itself is considered a weak learner, and the AdaBoost computes an output based on a collection of trees. This weighted averaging process leads to a more robust machine learning algorithm (i.e. one that is more robust in that its outputs on new unseen data samples should be similar to that used for testing and training, and that overtraining issues are reduced).  The process also results in what is called a strong learner - one that has a stronger separation between different categories of event than would be the case with a single tree, or indeed the individual features that are input into that tree.\n",
    "\n",
    "By now you should be very familiar with the iris data, both in terms of the 1D and 2D information, and what you can learn from the 1D distributions in terms of the ability to separate the three different types of iris from each other.  Here we take the next step to use a machine learning algorithm to simultaneously benefit from the 4-dimensional feature space to separate signal from background.\n",
    "\n",
    "## Task\n",
    "\n",
    "Train a classifier using the iris data and study the performance characteristics of this classifier in detail by working through this notebook.\n",
    "You should:\n",
    "- Add your name and student ID to this solution.\n",
    "- Work through the Iris data decision tree classification example in order to answer the following questions\n",
    "  - Using a train split of 0.5. Explore the effect of (a) changing the number of estimators, and (b) changing the tree depth, on the performance of the classifier. For this exercise tabulate results for including 10, 100, 500 and 1000 estimators (i.e. boosting iterations) and for tree depths of 1, 2, 3.  Measure performance by the fraction of mis-classified test examples.\n",
    "  - Repeat the above using a train split of 0.8.\n",
    "  - What is the configuration that leads to the least number of mis-classified examples.\n",
    "  - Why do you think, in detail, that any residual example(s) are mis-classified by the algorithm. If there are no-mis-classified examples, then does that concern you with regard to the use of this algorithm and the sample sizes used for train and test. You may wish to reflect on the earlier formative assignments and the notes to guide your response to this question.\n",
    "  - Reflect on the results you obtained for the train split size. Remark on any differences in performance that you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Name:        Henry Atkins\n",
    " * Student ID:  180196054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Solution\n",
    "\n",
    "Table exploring the effect of (a) changing the number of estimators, and (b) changing the tree depth, on the performance of the classifer. For this exercise tabulate results for including 10, 100, 500 and 1000 estimators (i.e. boosting iterations) and for tree depths of 1, 2, 3.  Measure performance by the fraction of mis-classified test examples.\n",
    "\n",
    "\n",
    "----------------\n",
    "Test split = 0.5\n",
    "| Tree Depth  | 10 iter  | 100 iter  | 500 iter  | 1000 iter |\n",
    "| ----------- | -------- | --------- | --------- | --------- |\n",
    "| 1           | 6.0      | 6.0       | 6.0       | 6.0       |\n",
    "| 2           | 3.0      | 3.0       | 3.0       | 3.0       |\n",
    "| 3           | 3.0      | 3.0       | 3.0       | 3.0       |\n",
    "\n",
    "----------------\n",
    "Test split = 0.8\n",
    "| Tree Depth  | 10 iter  | 100 iter  | 500 iter  | 1000 iter |\n",
    "| ----------- | -------- | --------- | --------- | --------- |\n",
    "| 1           | 1.0      | 3.0       | 3.0       | 3.0       |\n",
    "| 2           | 0.0      | 1.0       | 1.0       | 0.0       |\n",
    "| 3           | 0.0      | 1.0       | 1.0       | 1.0       |\n",
    "\n",
    "\n",
    "From the results obtained State the least number of mis-classified examples:\n",
    "\n",
    "   min(N-misclassified) = 0.8 Weighting, 10 Iterations, with Tree Depths 2, 3 as well as 1000 Iterations and 2 Trees.\n",
    "\n",
    "Reflect on this outcome\n",
    "\n",
    "   See Conclusion section below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "def misclassified(train_split_size, number_of_estimators, tree_depth):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0, train_size=train_split_size)\n",
    "    DT_clf  = DecisionTreeClassifier(max_depth=tree_depth, min_samples_leaf=1)\n",
    "    BDT_clf = AdaBoostClassifier(base_estimator=DT_clf, n_estimators=number_of_estimators).fit(X_train, y_train)\n",
    "    test_score  = BDT_clf.score(X_test, y_test)\n",
    "    incorrect = round((1-test_score)*100, 2)\n",
    "    return(incorrect)\n",
    "\n",
    "def compare(train_split_size, number_of_estimators, tree_depth):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0, train_size=train_split_size)\n",
    "    DT_clf  = DecisionTreeClassifier(max_depth=tree_depth, min_samples_leaf=1)\n",
    "    BDT_clf = AdaBoostClassifier(base_estimator=DT_clf, n_estimators=number_of_estimators).fit(X_train, y_train)\n",
    "    train_score = BDT_clf.score(X_train, y_train)\n",
    "    test_score  = BDT_clf.score(X_test, y_test)\n",
    "    return(round((1-train_score)*100, 3), round((1-test_score)*100, 3))\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified percentage for 0.5 training weightings:\n",
      "\n",
      "[['TD' '10' '100' '500' '1000']\n",
      " ['1' '8.0' '8.0' '8.0' '8.0']\n",
      " ['2' '4.0' '4.0' '4.0' '4.0']\n",
      " ['3' '4.0' '4.0' '4.0' '4.0']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Misclassified percentage for 0.5 training weightings:\" + \"\\n\")\n",
    "array50 = np.array([[\"TD\", \"10\", \"100\", \"500\", \"1000\"], \n",
    "                 [\"1\", misclassified(0.5, 10, 1), misclassified(0.5, 100, 1), misclassified(0.5, 500, 1), misclassified(0.5, 1000, 1)], \n",
    "                 [\"2\", misclassified(0.5, 10, 2), misclassified(0.5, 100, 2), misclassified(0.5, 500, 2), misclassified(0.5, 1000, 2)], \n",
    "                 [\"3\", misclassified(0.5, 10, 3), misclassified(0.5, 100, 3), misclassified(0.5, 500, 3), misclassified(0.5, 1000, 3)]])\n",
    "print(array50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified percentage for 0.8 training weightings:\n",
      "\n",
      "[['TD' '10' '100' '500' '1000']\n",
      " ['1' '3.33' '10.0' '10.0' '10.0']\n",
      " ['2' '0.0' '3.33' '3.33' '0.0']\n",
      " ['3' '0.0' '0.0' '3.33' '3.33']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Misclassified percentage for 0.8 training weightings:\" + \"\\n\")\n",
    "array80 = np.array([[\"TD\", \"10\", \"100\", \"500\", \"1000\"], \n",
    "                 [\"1\", misclassified(0.8, 10, 1), misclassified(0.8, 100, 1), misclassified(0.8, 500, 1), misclassified(0.8, 1000, 1)], \n",
    "                 [\"2\", misclassified(0.8, 10, 2), misclassified(0.8, 100, 2), misclassified(0.8, 500, 2), misclassified(0.8, 1000, 2)], \n",
    "                 [\"3\", misclassified(0.8, 10, 3), misclassified(0.8, 100, 3), misclassified(0.8, 500, 3), misclassified(0.8, 1000, 3)]])\n",
    "print(array80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum number of misclassified examples lies in the 0.8 test/training weighting table. There is zero mis-classified for:\n",
    "(Iterations, Tree Depth)=(10, 2),(10, 3), (1000, 2). \n",
    "\n",
    "This is due to the high amount of training the algorithm recieves, and the many trees. The Iterations which are greater than 100 (ie 500, 1000) show a large amount of overfitting to the training set. These score highly on the Training set, but not on the test set, as seen below. This shows that the training score is 100%, but the misclassified test set gets 1 wrong each. There is also another 0 mis-classified point, at (1000, 2) which appears to be another optimal point where the balence of Tree Depth and Iterations allows for accurate fitting, but not overfitting.  \n",
    "\n",
    "\n",
    "These results show some promise, a misclassification of 3.3% is good. But, there is evidence of overfitting, and so highly accurate parameter combinations should be considered critically. Please note there is an element of randomness here, as each time it is repeated, sometimes (500, 2) and (100, 3) show zero test errors. There are several oscillating values in the 80% training set, which indicates some random events. This could be due to a non-gaussian fit, or (as I belive) the \"random_state=0\" parameter. The analysis still holds, but some of the specific numbers mentioned may be different to quotes, most are oscillating between 0.00% and 3.33%.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over-Fitting Proof: (Train Errors, Test Errors)\n",
      "0.8, 500, 3:        (0.0, 3.333)\n",
      "0.8, 1000, 3:       (0.0, 3.333)\n",
      "Overfitting if Train Errors >> Test Errors\n"
     ]
    }
   ],
   "source": [
    "print(\"Over-Fitting Proof: (Train Errors, Test Errors)\")\n",
    "print(\"0.8, 500, 3:       \", compare(0.8, 500, 3))\n",
    "print(\"0.8, 1000, 3:      \", compare(0.8, 1000, 3))\n",
    "print(\"Overfitting if Train Errors >> Test Errors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
